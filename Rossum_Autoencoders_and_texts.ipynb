{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAA6qJSIDZ79"
   },
   "source": [
    "Welcome to google collab! If you are not familiar with the collab notebooks, I hope you will be positively surprised :)  \n",
    "\n",
    "Please:\n",
    "- Copy this notebook into your own drive (File->Save a copy to drive)\n",
    "- Look into this original notebook anyway you wish, it does contain our saved run\n",
    "- Run it \n",
    "  - And feel free to select Runtime->Change runtime type into a GPU accelerated workstation :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOP_zKFgK2kF"
   },
   "source": [
    "# Autoencoders - quick theory and live examples\n",
    "\n",
    "![autoencoder image](https://www.researchgate.net/publication/318204554/figure/fig1/AS:512595149770752@1499223615487/Autoencoder-architecture.png)   \n",
    "(Image from https://www.researchgate.net/publication/318204554_Intelligent_condition_monitoring_method_for_bearing_faults_from_highly_compressed_measurements_using_sparse_over-complete_features/figures?lo=1)  \n",
    "\n",
    "  \n",
    "Autoencoder is a model, that (is trained to) map outputs to be as similar as inputs as possible.  \n",
    "Keys:\n",
    "- Unsupervised learning\n",
    "- The point is in the architecture, because it is not an identity function\n",
    " - But we can say identity-like with 'bottleneck'\n",
    "- has 2 parts:\n",
    " - encoder (maps input to inner representation)\n",
    " - decoder (maps inner representation to output)\n",
    "- both parts trained together to reduce input-output error\n",
    "- were used for pretraining a deeper models (as in image, each part trained alone)\n",
    "  - ![alt text](https://www.researchgate.net/profile/Francois_Pachet/publication/319524552/figure/fig21/AS:535791140708363@1504753970877/Stacked-autoencoders-architecture_W640.jpg)\n",
    "- now used for dimensionality reduction and other usecases (covered here as ecamples)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dc6muLM_o2r4"
   },
   "source": [
    "## Autoencoder live demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "SjNolFi-nIML",
    "outputId": "d009d09e-a9c0-43f6-b045-4404d8ecdbbd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f213405d7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rn-3gwGknYnT",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f213405d4a8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not exactly an autoencoder, only when you think that the input and output are images and the bottleneck is, that it is able to use just positions.\n",
    "IFrame('https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "W31326syoXro",
    "outputId": "dffc7bdf-42e4-45f6-cd6c-4089ca920438"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://magenta.tensorflow.org/assets/sketch_rnn_demo/multi_vae.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f213405d710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('https://magenta.tensorflow.org/assets/sketch_rnn_demo/multi_vae.html', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "q_6uaJOloY99",
    "outputId": "5e95bcdf-80ad-48bf-9365-ef0bd6744d0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://transcranial.github.io/keras-js/#/mnist-vae\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f213405d780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('https://transcranial.github.io/keras-js/#/mnist-vae', width=900, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUmgvGxrxLYc"
   },
   "source": [
    "# Autoencoders and text data - Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3W4niBhfVyg"
   },
   "source": [
    "### What we would like to cover:\n",
    "- Autoencoders usage on texts with word by word representation\n",
    "- **How to work with autoencoders, how to use them and analyze them. (As opposed to 'how to build an anutoencoder to reach a given goal')**\n",
    "  - we will specifically aim to use the most basic autoencoder (see image at the top)\n",
    "  - ... and do not scroll to the bottom immediately, there are spoilers :)\n",
    "    \n",
    "Note:\n",
    "- The usual flow of experimental work would be to experiment (proof of work) on training data and validate/test on testing data.\n",
    "  - Note, that here we do only experiments on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMW1df_wpXV"
   },
   "source": [
    "## Preparation of the environment + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "Nsn616Q-wn0w",
    "outputId": "e74f580a-e452-4420-c752-b68cb9ed04d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bhtsne\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/32/3e25093337ab8811e0c3ebd89ad60fd6ca19ef9d7cebb645454f2bc59eaf/bhtsne-0.1.9.tar.gz (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 5.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bhtsne) (1.14.6)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from bhtsne) (0.29.5)\n",
      "Building wheels for collected packages: bhtsne\n",
      "  Building wheel for bhtsne (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d3/11/da/b469506296148e4a04bb3bd5083052a2c5d44709851ed17c21\n",
      "Successfully built bhtsne\n",
      "Installing collected packages: bhtsne\n",
      "Successfully installed bhtsne-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install bhtsne  # for visualizations with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8QNksNMStB00",
    "outputId": "e198c203-2064-4ae8-e027-6ab5b8a59869"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from six import string_types, reraise\n",
    "from copy import copy\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chardet\n",
    "from bhtsne import tsne  #https://github.com/dominiek/python-bhtsne\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "\n",
    "class tempmap(np.memmap):\n",
    "  \"\"\"\n",
    "  Just a helper class for memmap numpy arrays to stay in the temporary directory.\n",
    "  \"\"\"\n",
    "  def __new__(subtype, dtype=np.uint8, mode='w+', offset=0,\n",
    "              shape=None, order='C'):\n",
    "      ntf = tempfile.NamedTemporaryFile()\n",
    "      self = np.memmap.__new__(subtype, ntf, dtype, mode, offset, shape, order)\n",
    "      self.temp_file_obj = ntf\n",
    "      return self\n",
    "\n",
    "  def __del__(self):\n",
    "      if hasattr(self,'temp_file_obj') and self.temp_file_obj is not None:\n",
    "          self.temp_file_obj.close()\n",
    "          # del self.temp_file_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IobMq8uywwCC"
   },
   "source": [
    "## IMDB Dataset download & inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIdQhifghkht"
   },
   "source": [
    "At this point, any dataset can be used, we have decided to use a dataset of IMDB movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "k9n6CaNJthOD",
    "outputId": "e45b154a-40da-434d-dafb-a17b91a1f63f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-22 11:33:58--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
      "\n",
      "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  1.04MB/s    in 1m 48s  \n",
      "\n",
      "2019-02-22 11:35:49 (760 KB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar xfz aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3VtNgqUUtlLb",
    "outputId": "e0547dda-e4bd-42de-cb5c-30be6fc541ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n",
      "20000 words in vocabulary\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary: Loads all words used, starting by the most frequent.\n",
    "# We need the vocabulary only for displaying the results.\n",
    "vocab_cut = 20000 # Keep only most frequent words rather than all\n",
    "\n",
    "with open('aclImdb/imdb.vocab', \"rb\") as f:\n",
    "    rawdata=f.read()\n",
    "    \n",
    "    # automatic inspect of the encoding and data, in case we forget :)\n",
    "    print(chardet.detect(rawdata))\n",
    "    f.seek(0)\n",
    "    vocab = [word.decode(\"utf8\").rstrip() for word in f]\n",
    "    \n",
    "    # Just saving memory - the long tail occurs too few times\n",
    "    # for the model to learn anything anyway\n",
    "    vocab = vocab[:vocab_cut]\n",
    "    print('%d words in vocabulary' % (len(vocab),))\n",
    "    print(type(vocab[0]))\n",
    "\n",
    "# append some number to the vocabulary (for disaplying resons, does not affect training)\n",
    "for i in range(99):\n",
    "  vocab.append(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bMzDBv7xAbk"
   },
   "source": [
    "## Nonautoencoder baseline and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oixos7t2xrTi"
   },
   "source": [
    "The text processing should employ tokenization of the text and some helper functions like deaccenting, because autoencoders (and other text procesing algorithms and models in ML) do not work with text naturally.  \n",
    "  \n",
    " Generally we have more options:  \n",
    " 1. Embedd each word into defined dimensional space by a trained embedding.\n",
    " 2. Put onehot representation of each word or character into the network\n",
    " 3. Use 'feature vectors' for each word\n",
    "   \n",
    "For our first try with ML model, lets use feature vectors.\n",
    "- the feature vector generator does not use memory as much as embeddings\n",
    "- feature vectors are 'fair' to our model, meaning that embeddings are usually pretrained and will already bear more information\n",
    "- every word has its feature vector (as opposed to the fact, that not all words have an embedding)\n",
    "- character level models can be a tiny bit harder to understand or train\n",
    "  \n",
    "Feature vector function used here ('features_from_text') is usually used for a text analysis task, because it does count the number of lowercase/uppercase letter/digits and character frequencies.  \n",
    "\n",
    "With this function we are, moreover, able to lookup back into a dictionary (will be shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Fh_Utgy9tsKN",
    "outputId": "159fd698-7fda-464d-f59c-fbf06241e47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets test our code:\n",
      "Does tokenization leave a word in a vocabulary alone?: ['cliché']\n",
      "Word 'cliché' is, for example, translated as [0.3, 0.0, 0.3, 0.3, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.05, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Words 'cliché' is deaccented as cliche\n"
     ]
    }
   ],
   "source": [
    "def text_tokens(text):\n",
    "  \"\"\"\n",
    "  Tokenization is the proces that turns a text sentence into a sequence of tokens,\n",
    "  i.e. decides what is a word and what is a separator.\n",
    "  \"\"\"  \n",
    "  text = re.sub(\"\\\\s\", \" \", text)\n",
    "  text = re.sub(\"<br/>\", \" \", text)\n",
    "  text = re.sub(\"<br />\", \" \", text)\n",
    "  text = re.sub(\"<br>\", \" \", text)\n",
    "  #text = re.sub(\"[^a-zA-Z' ]\", \"\", text)\n",
    "  tokens = re.split('(\\W+)', text) # text.split(' ')\n",
    "  tokens = [re.sub(\" \", \"\", token) for token in tokens if token not in [' ', '']]\n",
    "  return tokens\n",
    "\n",
    "# This is our vocabulary of characters we will work with, other will be forgotten\n",
    "default_char_list = u' abcdefghijklmnopqrstuvwxyz0123456789,.-+:/%?$£€#()&\\''\n",
    "default_char_vocab = {letter: i for i, letter in enumerate(list(default_char_list))}\n",
    "\n",
    "\n",
    "def remove_accents(input_str):\n",
    "  \"\"\"\n",
    "  We do not want accents and special characters, so lets normalize the text.\n",
    "  \"\"\"\n",
    "  try:\n",
    "      # variant:\n",
    "      # unicoded = unicode(input_char)\n",
    "      # # .decode('utf-8', 'ignore')\n",
    "      # nfkd_form = unicodedata.normalize('NFKD', unicoded)\n",
    "      # only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "      # if len(only_ascii) <= 0:\n",
    "      #    return nfkd_form\n",
    "\n",
    "      if len(input_str) == 0:\n",
    "        return u\"\"\n",
    "      unistr = input_str.replace(u'\\xf8', u'o')\n",
    "      unistr = unistr.replace(u'\\xd8', u'O')\n",
    "\n",
    "      unistr = \"\".join(aChar\n",
    "                     for aChar in unicodedata.normalize(\"NFD\", unistr)\n",
    "                     if not unicodedata.combining(aChar))\n",
    "      return unistr\n",
    "  except Exception as e:\n",
    "      print(\"at {}\".format(input_str))\n",
    "      reraise(type(e), e, sys.exc_info()[2])\n",
    "\n",
    "        \n",
    "def base_text_features(text, features=['len', 'upper', 'lower', 'alpha', 'digit'],\n",
    "                       scale=20,\n",
    "                       char_vocab=default_char_vocab):\n",
    "  \"\"\"\n",
    "  Function, that extract features from the given text based on given parameters.\n",
    "  If character vocabulary is given, the features contain character frequencies.\n",
    "  \"\"\"\n",
    "  def text_histogram(text, char_vocab=default_char_vocab,\n",
    "                     char_vocab_make_lower=True):\n",
    "      hist = [0] * len(default_char_vocab.keys())\n",
    "      for letter in text:\n",
    "          if char_vocab_make_lower:\n",
    "              luse = letter.lower()\n",
    "          else:\n",
    "              luse = letter\n",
    "          luse = remove_accents(luse)\n",
    "          if luse in default_char_vocab:\n",
    "              hist[default_char_vocab[luse]] += 1\n",
    "      return hist\n",
    "\n",
    "  def count_uppers(text):\n",
    "      return sum([letter.isupper() for letter in text])\n",
    "\n",
    "  def count_lowers(text):\n",
    "      return sum([letter.islower() for letter in text])\n",
    "\n",
    "  def count_alphas(text):\n",
    "      return sum([letter.isalpha() for letter in text])\n",
    "\n",
    "  def count_digits(text):\n",
    "      return sum([letter.isdigit() for letter in text])\n",
    "\n",
    "  use_cases = {\n",
    "      'len': len,\n",
    "      'upper': count_uppers,\n",
    "      'lower': count_lowers,\n",
    "      'alpha': count_alphas,\n",
    "      'digit': count_digits,\n",
    "  }\n",
    "  repr = [use_cases[feature](text) for feature in features]\n",
    "  if char_vocab is not None:\n",
    "      repr.extend(text_histogram(text, default_char_vocab))\n",
    "\n",
    "  if scale is not None:\n",
    "      for i in range(len(repr)):\n",
    "          repr[i] = min(repr[i] / scale, 1.0)\n",
    "\n",
    "  return repr\n",
    "\n",
    "\n",
    "def features_from_text(text, values_scales=[100.0], scale=20):\n",
    "  \"\"\"\n",
    "  Lets extract features from the whole word, beginning of a word and ending of a word.\n",
    "  (Also if the given word is a number, lets scale it into [0,1] interval;\n",
    "  this should help the network with text, that does come after a number as '1 st')\n",
    "  \"\"\"\n",
    "  try:\n",
    "      xtextasval = float(text.replace(\" \", \"\").replace(\"%\", \"\"))\n",
    "      xtextisval = 1.0\n",
    "      assert np.isfinite(xtextasval)\n",
    "  except:\n",
    "      xtextasval = 0.0\n",
    "      xtextisval = 0.0\n",
    "  if xtextisval > 0.0:  # is actually a value\n",
    "      xtextasval = [min(xtextasval / scale, 1.0) for scale in values_scales]\n",
    "  else:\n",
    "      xtextasval = [0.0] * len(values_scales)\n",
    "\n",
    "  allfeats = base_text_features(text, scale=scale, features=['len', 'upper', 'lower', 'alpha', 'digit'])\n",
    "  if len(text) <= 1:\n",
    "      # just if we use the histograms for first two letters and last two letters, what to do in smaller\n",
    "      text_to_handle = \" \" + text + \" \"\n",
    "  else:\n",
    "      text_to_handle = text\n",
    "  begfeats = base_text_features(text_to_handle[0:2], scale=scale, features=['upper', 'lower', 'alpha', 'digit'])\n",
    "  endfeats = base_text_features(text_to_handle[-2:0], scale=scale, features=['upper', 'lower', 'alpha', 'digit'])\n",
    "\n",
    "  return allfeats + begfeats + endfeats + xtextasval + [xtextisval]\n",
    "\n",
    "\n",
    "print(\"Lets test our code:\")\n",
    "print(\"Does tokenization leave a word in a vocabulary alone?: {}\".\n",
    "      format(text_tokens(vocab[1552])))\n",
    "print(u\"Word '{}' is, for example, translated as {} \".format(vocab[1552], features_from_text(vocab[1552])))\n",
    "print(u\"Words '{}' is deaccented as {}\".format(vocab[1552], remove_accents(vocab[1552])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BpXom_uhCHS"
   },
   "source": [
    "When using any form of inner representations, we would need to have a function, that rewrites the inner representation into an existing word to be understandable for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "zErw9LwDv5l6",
    "outputId": "93371930-f6ee-4601-c954-dcdb39027906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A test that/how the vocabulary search works: \n",
      "\n",
      "((0, 'the'), 0.0)\n",
      "((15379, 'unprofessional'), 0.0001149425287356322)\n",
      "((948, 'potential'), 0.0)\n",
      "\n",
      "Idea: Try to think about the feature vector as a hash and find collisions in the provided hashing over the vocabulary\n",
      "Idea: Try the code with bigger/smaller vocabulary\n"
     ]
    }
   ],
   "source": [
    "def vocab_finder(item, vocab, key_f=None, apply_on_item_too=True):\n",
    "  \"\"\"\n",
    "  A modular function to allow for searching an item against vocabulary of items\n",
    "  based on the minimal distance. \n",
    "  The modularity allows for applying a function over the vocabulary\n",
    "  or query items.\n",
    "  \"\"\"\n",
    "  if apply_on_item_too:\n",
    "      fitem = key_f(item) if key_f else item\n",
    "  else:\n",
    "      fitem = item\n",
    "  mindiff = None\n",
    "  minitem = None\n",
    "  \n",
    "  for v, vitem in enumerate(vocab):\n",
    "      fvitem = key_f(vitem) if key_f else vitem\n",
    "      \n",
    "      diff = np.mean(np.square(np.asarray(fitem) - np.asarray(fvitem)))        \n",
    "      if mindiff is None or diff < mindiff:\n",
    "          mindiff = diff\n",
    "          minitem = (v, vitem)\n",
    "  return minitem, mindiff\n",
    "\n",
    "\n",
    "print(\"A test that/how the vocabulary search works: \\n\")\n",
    "print(vocab_finder(\"the\", vocab, features_from_text))\n",
    "print(vocab_finder(\"nonsensualword\", vocab, features_from_text))\n",
    "print(vocab_finder(\"potential\", vocab, features_from_text))\n",
    "print(\"\")\n",
    "print(\"Idea: Try to think about the feature vector as a hash and find collisions in the provided hashing over the vocabulary\")\n",
    "print(\"Idea: Try the code with bigger/smaller vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFV-kb6xxmOV"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B51wG17j0NjS"
   },
   "source": [
    "We will load a small subset of the dataset (can be increased for speed tradeoff :) )  and process it together with vocabulary with our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "yYh-LIFRv9Ad",
    "outputId": "a2bd62ac-fdf1-47e1-80f7-7e7b3caa7497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "The loaded dataset has length 5000 \n",
      " sample tokenized sentence from dataset is: ['Absolutely', 'dreadful', 'Mexican', 'film', 'supposedly', 'based', 'on', 'a', 'short', 'story', 'by', 'Edgar', 'Allan', 'Poe', 'about', 'a', 'newsman', 'wanting', 'to', 'go', 'into', 'the', 'confines', 'of', 'an', 'asylum', 'hidden', 'in', 'the', 'woods', 'to', 'write', 'a', 'story', 'about', 'how', 'it', 'works', ',', 'etc', '...', 'When', 'our', 'hero', ',', 'Gaston', ',', 'is', 'given', 'the', 'grand', 'show', 'by', 'Dr', '.', 'Maillard', ',', 'head', 'of', 'the', 'asylum', ',', 'we', 'see', 'all', 'kinds', 'of', 'things', 'which', 'are', 'suppose', 'to', 'be', 'horrific', ',', 'such', 'as', 'men', 'hanging', 'around', 'long', 'in', 'a', 'dungeon', ',', 'and', 'comedic', ',', 'such', 'as', 'our', 'hero', 'being', 'joked', 'upon', 'by', 'soldiers', 'as', 'he', 'climbs', 'down', 'a', 'ladder', 'hanging', 'over', 'the', 'side', 'of', 'a', 'building', '.', 'Then', 'there', 'is', 'one', 'sight', 'which', 'might', 'have', 'been', 'meant', 'to', 'be', 'both', ':', 'a', 'human', 'man', 'dressed', 'as', 'a', 'chicken', ',', 'yes', ',', 'that', \"'\", 's', 'right', 'a', 'chicken', ',', 'that', 'pecks', 'around', 'the', 'ground', 'for', 'chicken', 'feed', '.', 'The', 'scene', 'was', 'to', 'be', 'a', 'comedic', 'highlight', 'of', 'the', 'film', ',', 'but', ',', 'at', 'least', 'for', 'me', ',', 'it', 'was', 'the', 'film', \"'\", 's', 'low', 'point', 'and', 'really', 'most', 'revolting', 'when', 'you', 'considered', 'that', 'grown', 'men', 'and', 'women', 'thought', 'this', 'might', 'even', 'be', 'remotely', 'entertaining', '.', 'Ah', '!', 'That', 'is', 'indeed', 'the', 'real', 'horror', 'that', 'is', 'Dr', '.', 'Tarr', 'and', 'his', 'Legion', 'of', 'Name', 'Changes', '.', 'And', 'that', 'brings', 'me', 'to', 'this', 'salient', 'fact', 'about', 'the', 'film', 'which', 'is', 'most', 'films', 'that', 'undergo', 'multiple', 'title', 'changes', 'usually', 'have', 'some', 'kind', 'of', 'serious', 'problem', '.', 'Yes', ',', 'this', 'is', 'obvious', ',', 'but', 'some', 'have', 'distribution', 'problems', 'and', 'others', ',', 'of', 'which', 'this', 'is', 'one', ',', 'have', 'numerous', 'title', 'changes', 'so', 'that', 'someone', 'might', 'unsuspectingly', 'buy', 'the', 'same', 'garbage', 'more', 'than', 'once', '.', 'This', 'is', 'definitely', 'garbage', '.', 'It', 'has', 'very', 'little', 'going', 'for', 'it', '.', 'The', 'only', 'performer', 'worth', 'having', 'a', 'look', 'at', 'is', 'Claudio', 'Brook', 'as', 'the', 'head', 'of', 'the', 'asylum', '.', 'He', 'is', 'one', 'huge', 'slab', 'of', 'ham', 'as', 'he', 'laughs', 'maniacally', ',', 'bellows', 'orders', ',', 'sashays', 'with', 'sword', 'in', 'hand', ',', 'and', 'praises', 'the', 'chicken', '.', 'I', 'got', 'so', 'tired', 'of', 'hearing', 'him', 'talk', 'about', 'the', '\"', 'soothing', 'system', '\"', 'as', 'his', 'means', 'to', 'cure', 'the', 'mentally', 'sick', '.', 'What', 'a', 'bunch', 'of', 'ludicrosity', '(', 'Hey', ',', 'a', 'film', 'like', 'this', 'with', 'a', 'script', 'like', 'this', 'deserves', 'this', 'kind', 'of', 'word', ').', 'It', 'won', \"'\", 't', 'take', 'you', 'long', 'to', 'figure', 'out', 'what', 'is', 'going', 'on', 'in', 'the', 'asylum', 'nor', 'will', 'it', 'be', 'any', 'more', 'interesting', '.', 'Cinematic', 'chicken', 'scratch', '!']\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dirname, train_set_slice_size=5000):\n",
    "  \"\"\"\n",
    "  Loads and tokenizes a dataset into a list.\n",
    "  Label loading included, even though we will not use it for autoencoders.\n",
    "  Set train_set_slice_size to 500 for fast tryout.\n",
    "  \"\"\"\n",
    "  X, y = [], []\n",
    "  # Review files: neg/0_3.txt neg/10000_4.txt neg/10001_4.txt ...\n",
    "  for y_val, y_label in enumerate(['neg', 'pos']):\n",
    "      y_dir = os.path.join(dirname, y_label)\n",
    "      for fname in os.listdir(y_dir):\n",
    "          fpath = os.path.join(y_dir, fname)\n",
    "          with open(fpath) as f:\n",
    "              tokens = text_tokens(f.read())            \n",
    "          X.append(tokens)            \n",
    "          y.append(y_val)  # 0 for 'neg', 1 for 'pos'\n",
    "\n",
    "          if len(X) >= train_set_slice_size:\n",
    "            break\n",
    "      if len(X) >= train_set_slice_size:\n",
    "        break\n",
    "  return X, y\n",
    "\n",
    "print(\"loading dataset\")\n",
    "X_train_orig, y_train = load_dataset('aclImdb/train/')\n",
    "print(\"The loaded dataset has length {} \\n sample tokenized sentence from dataset is: {}\"\n",
    "      .format(len(X_train_orig), X_train_orig[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "WFYhYBGWv_wh",
    "outputId": "bce57bb6-97e7-482b-ea80-0b95c9e10991"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/5000 [00:00<01:45, 47.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: creating dataset of floats from dataset of tokenized texts:\n",
      "Padded shape of training set is [5000, 1654, 174]\n",
      "Running embedding function over the dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:10<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared with embeddings into shape (5000, 1654, 174)\n",
      "Running embedding function over the vocabulary ...\n",
      "Done ...\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_with_embeddings(vocab,\n",
    "                                    embedd_f=features_from_text,\n",
    "                                    X_train=X_train_orig):\n",
    "  \"\"\"\n",
    "  Uses selected embedding function (which is here our feature vector function)\n",
    "  and turns dataset and vocabulary into its embedding.  \n",
    "  \"\"\"\n",
    "  shape = [len(X_train), max(len(text) for text in X_train)] + [len(embedd_f(\"default\"))]\n",
    "\n",
    "  print(\"Padded shape of training set is {}\".format(shape))\n",
    "  print(\"Running embedding function over the dataset ...\")\n",
    "\n",
    "  train_data = tempmap(shape=tuple(shape), dtype=np.float32) #np.zeros(shape)\n",
    "  for i, sentence in tqdm(enumerate(X_train), total=len(X_train)):\n",
    "      for j, word in enumerate(sentence):\n",
    "          train_data[i, j, :] = embedd_f(word)\n",
    "  print(\"Dataset prepared with embeddings into shape {}\".\n",
    "        format(train_data.shape))\n",
    "  \n",
    "  print(\"Running embedding function over the vocabulary ...\")\n",
    "  vocab_embedds = [(embedd_f(word), word) for word in vocab]\n",
    "  print(\"Done ...\")\n",
    "  return train_data, vocab_embedds\n",
    "\n",
    "print(\"Embeddings: creating dataset of floats from dataset of tokenized texts:\")\n",
    "train_data, vocab_embedds = prepare_dataset_with_embeddings(vocab,\n",
    "                                                             features_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYhtQrM50f_P"
   },
   "source": [
    "## Autoencoders on embeddings/feature vectors:\n",
    "We will use the most simple autoencoder architecture, that just flattens the representation, runs it through a bottleneck with a dropout, then decodes and inflates again.  \n",
    "![architecture](https://drive.google.com/uc?export=view&id=1KMfr0cLY3cvp9KbqOjiQQZM9Ae7LK7MP)  \n",
    "(Image source: http://uksim.info/isms2016/CD/data/0665a174.pdf)  \n",
    "  \n",
    "The dropout mechanism  \n",
    "![dropout](https://drive.google.com/uc?export=view&id=1nYxmBMIx3QPFqBw-Q1hJOr-Hvi7ASx7d)  \n",
    "\n",
    "(Dropout illustration from https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3519
    },
    "colab_type": "code",
    "id": "C7PEGOnXwIHl",
    "outputId": "a8ef3f8f-b696-4c8f-b7e7-7c0c36c13831",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding_dim': 800, 'train_data_shape': (5000, 1654, 174)}\n",
      "Architecture will compress data to 0.22988505747126436 fraction\n",
      "WARNING:tensorflow:From /home/zhukovgreen/.cache/pypoetry/virtualenvs/mlprague2019-py3.7/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/zhukovgreen/.cache/pypoetry/virtualenvs/mlprague2019-py3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(4900, 20, 174)\n",
      "(100, 20, 174)\n",
      "WARNING:tensorflow:From /home/zhukovgreen/.cache/pypoetry/virtualenvs/mlprague2019-py3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4900 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "4900/4900 [==============================] - 1s 268us/step - loss: 6.9342e-04 - val_loss: 3.9070e-04\n",
      "Epoch 2/100\n",
      "4900/4900 [==============================] - 0s 79us/step - loss: 3.5612e-04 - val_loss: 2.3964e-04\n",
      "Epoch 3/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 2.3352e-04 - val_loss: 1.6594e-04\n",
      "Epoch 4/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 1.9204e-04 - val_loss: 1.4423e-04\n",
      "Epoch 5/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 1.7085e-04 - val_loss: 1.2899e-04\n",
      "Epoch 6/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 1.5428e-04 - val_loss: 1.1533e-04\n",
      "Epoch 7/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 1.4169e-04 - val_loss: 1.0596e-04\n",
      "Epoch 8/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 1.3281e-04 - val_loss: 9.8473e-05\n",
      "Epoch 9/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 1.2429e-04 - val_loss: 9.2412e-05\n",
      "Epoch 10/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 1.1815e-04 - val_loss: 8.7672e-05\n",
      "Epoch 11/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 1.1247e-04 - val_loss: 8.2924e-05\n",
      "Epoch 12/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 1.0670e-04 - val_loss: 7.9382e-05\n",
      "Epoch 13/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 1.0271e-04 - val_loss: 7.5898e-05\n",
      "Epoch 14/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 9.9624e-05 - val_loss: 7.2567e-05\n",
      "Epoch 15/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 9.4419e-05 - val_loss: 6.9655e-05\n",
      "Epoch 16/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 9.1540e-05 - val_loss: 6.6828e-05\n",
      "Epoch 17/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 8.7962e-05 - val_loss: 6.4800e-05\n",
      "Epoch 18/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 8.4808e-05 - val_loss: 6.2084e-05\n",
      "Epoch 19/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 8.1899e-05 - val_loss: 5.9684e-05\n",
      "Epoch 20/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 7.8805e-05 - val_loss: 5.7953e-05\n",
      "Epoch 21/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 7.8101e-05 - val_loss: 5.5678e-05\n",
      "Epoch 22/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 7.4849e-05 - val_loss: 5.4220e-05\n",
      "Epoch 23/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 7.3007e-05 - val_loss: 5.2225e-05\n",
      "Epoch 24/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 7.1075e-05 - val_loss: 5.0901e-05\n",
      "Epoch 25/100\n",
      "4900/4900 [==============================] - 0s 80us/step - loss: 7.0317e-05 - val_loss: 5.0180e-05\n",
      "Epoch 26/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 6.7943e-05 - val_loss: 4.7952e-05\n",
      "Epoch 27/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 6.7021e-05 - val_loss: 4.6660e-05\n",
      "Epoch 28/100\n",
      "4900/4900 [==============================] - 0s 79us/step - loss: 6.5957e-05 - val_loss: 4.7005e-05\n",
      "Epoch 29/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 6.4534e-05 - val_loss: 4.4985e-05\n",
      "Epoch 30/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 6.3725e-05 - val_loss: 4.4368e-05\n",
      "Epoch 31/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 6.2957e-05 - val_loss: 4.3181e-05\n",
      "Epoch 32/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 6.2597e-05 - val_loss: 4.2670e-05\n",
      "Epoch 33/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 6.1416e-05 - val_loss: 4.1749e-05\n",
      "Epoch 34/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 6.0489e-05 - val_loss: 4.1205e-05\n",
      "Epoch 35/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 5.9887e-05 - val_loss: 3.9607e-05\n",
      "Epoch 36/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.9178e-05 - val_loss: 3.9160e-05\n",
      "Epoch 37/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.7585e-05 - val_loss: 3.8175e-05\n",
      "Epoch 38/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 5.7031e-05 - val_loss: 3.8880e-05\n",
      "Epoch 39/100\n",
      "4900/4900 [==============================] - 0s 79us/step - loss: 5.6913e-05 - val_loss: 3.6881e-05\n",
      "Epoch 40/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 5.6379e-05 - val_loss: 3.7085e-05\n",
      "Epoch 41/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 5.5908e-05 - val_loss: 3.6897e-05\n",
      "Epoch 42/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 5.5249e-05 - val_loss: 3.5412e-05\n",
      "Epoch 43/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 5.4357e-05 - val_loss: 3.5899e-05\n",
      "Epoch 44/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 5.4267e-05 - val_loss: 3.4502e-05\n",
      "Epoch 45/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 5.3593e-05 - val_loss: 3.4581e-05\n",
      "Epoch 46/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 5.3739e-05 - val_loss: 3.4349e-05\n",
      "Epoch 47/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.2808e-05 - val_loss: 3.3642e-05\n",
      "Epoch 48/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 5.3150e-05 - val_loss: 3.3287e-05\n",
      "Epoch 49/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 5.2407e-05 - val_loss: 3.2807e-05\n",
      "Epoch 50/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.2131e-05 - val_loss: 3.2477e-05\n",
      "Epoch 51/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 5.1434e-05 - val_loss: 3.1904e-05\n",
      "Epoch 52/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.1571e-05 - val_loss: 3.1870e-05\n",
      "Epoch 53/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.0426e-05 - val_loss: 3.1679e-05\n",
      "Epoch 54/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 5.0771e-05 - val_loss: 3.1290e-05\n",
      "Epoch 55/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.0416e-05 - val_loss: 3.0971e-05\n",
      "Epoch 56/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 5.1122e-05 - val_loss: 3.1168e-05\n",
      "Epoch 57/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.9463e-05 - val_loss: 2.9834e-05\n",
      "Epoch 58/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.9954e-05 - val_loss: 3.0042e-05\n",
      "Epoch 59/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.9285e-05 - val_loss: 3.0501e-05\n",
      "Epoch 60/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.9619e-05 - val_loss: 2.9051e-05\n",
      "Epoch 61/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.8062e-05 - val_loss: 2.9529e-05\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.9295e-05 - val_loss: 2.8625e-05\n",
      "Epoch 63/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.7995e-05 - val_loss: 2.8557e-05\n",
      "Epoch 64/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.7594e-05 - val_loss: 2.8575e-05\n",
      "Epoch 65/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.7909e-05 - val_loss: 2.7982e-05\n",
      "Epoch 66/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.8156e-05 - val_loss: 2.8408e-05\n",
      "Epoch 67/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.7820e-05 - val_loss: 2.7422e-05\n",
      "Epoch 68/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.8471e-05 - val_loss: 2.7706e-05\n",
      "Epoch 69/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.8468e-05 - val_loss: 2.7311e-05\n",
      "Epoch 70/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.7136e-05 - val_loss: 2.7607e-05\n",
      "Epoch 71/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.6808e-05 - val_loss: 2.7183e-05\n",
      "Epoch 72/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.7223e-05 - val_loss: 2.7351e-05\n",
      "Epoch 73/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.6933e-05 - val_loss: 2.7051e-05\n",
      "Epoch 74/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.6936e-05 - val_loss: 2.6441e-05\n",
      "Epoch 75/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.6577e-05 - val_loss: 2.7021e-05\n",
      "Epoch 76/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.6996e-05 - val_loss: 2.6268e-05\n",
      "Epoch 77/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.6477e-05 - val_loss: 2.6480e-05\n",
      "Epoch 78/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.5948e-05 - val_loss: 2.6133e-05\n",
      "Epoch 79/100\n",
      "4900/4900 [==============================] - 0s 73us/step - loss: 4.6405e-05 - val_loss: 2.6199e-05\n",
      "Epoch 80/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.5516e-05 - val_loss: 2.6137e-05\n",
      "Epoch 81/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.5988e-05 - val_loss: 2.6896e-05\n",
      "Epoch 82/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.6471e-05 - val_loss: 2.5934e-05\n",
      "Epoch 83/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.5700e-05 - val_loss: 2.5615e-05\n",
      "Epoch 84/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.5353e-05 - val_loss: 2.6373e-05\n",
      "Epoch 85/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.5405e-05 - val_loss: 2.5581e-05\n",
      "Epoch 86/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 4.4898e-05 - val_loss: 2.5141e-05\n",
      "Epoch 87/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.5214e-05 - val_loss: 2.5094e-05\n",
      "Epoch 88/100\n",
      "4900/4900 [==============================] - 0s 78us/step - loss: 4.4843e-05 - val_loss: 2.5073e-05\n",
      "Epoch 89/100\n",
      "4900/4900 [==============================] - 0s 77us/step - loss: 4.5091e-05 - val_loss: 2.4942e-05\n",
      "Epoch 90/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.5514e-05 - val_loss: 2.5000e-05\n",
      "Epoch 91/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.4306e-05 - val_loss: 2.4561e-05\n",
      "Epoch 92/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.4215e-05 - val_loss: 2.4443e-05\n",
      "Epoch 93/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.4508e-05 - val_loss: 2.4618e-05\n",
      "Epoch 94/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.4475e-05 - val_loss: 2.4100e-05\n",
      "Epoch 95/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.5073e-05 - val_loss: 2.3780e-05\n",
      "Epoch 96/100\n",
      "4900/4900 [==============================] - 0s 74us/step - loss: 4.4252e-05 - val_loss: 2.4514e-05\n",
      "Epoch 97/100\n",
      "4900/4900 [==============================] - 0s 75us/step - loss: 4.3883e-05 - val_loss: 2.4285e-05\n",
      "Epoch 98/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.4045e-05 - val_loss: 2.4411e-05\n",
      "Epoch 99/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.4374e-05 - val_loss: 2.3964e-05\n",
      "Epoch 100/100\n",
      "4900/4900 [==============================] - 0s 76us/step - loss: 4.4005e-05 - val_loss: 2.3966e-05\n",
      "Predicted data on training and test\n"
     ]
    }
   ],
   "source": [
    "def basic_autoencoder_architecture(window_size, encoding_dim,\n",
    "                                   train_data_shape):\n",
    "  \"\"\"  \n",
    "  Copied more or less from the keras blog:\n",
    "  https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "  \n",
    "  encoding_dim is the size of our encoded representations\n",
    "  \"\"\"\n",
    "  inp_shape = window_size * train_data_shape[-1]  # flattened representation size\n",
    "\n",
    "  print(\"Architecture will compress data to {} fraction\"\n",
    "        .format(encoding_dim / inp_shape))\n",
    "\n",
    "  # this is our input placeholder\n",
    "  input_data = Input(shape=(window_size, train_data_shape[-1], ))\n",
    "  input_flat = Flatten()(input_data)\n",
    "  # \"encoded\" is the encoded representation of the input\n",
    "  encoded = Dense(encoding_dim, activation='tanh')(Dropout(0.15)(input_flat))\n",
    "  \n",
    "  # defined order of layers to apply when building atuencoder.\n",
    "  # If we instantiate the layers now and then only apply them functionally,\n",
    "  # they will share their (trained) weights\n",
    "  decoder_layers = [Dense(inp_shape, activation='tanh'),\n",
    "                    Reshape((window_size, train_data_shape[-1]))]\n",
    "  def apply_decoder_layers(inner_input):\n",
    "    out = inner_input\n",
    "    for layer in decoder_layers:\n",
    "      out = layer(out)\n",
    "    return out\n",
    "\n",
    "  # this model maps an input to its reconstruction\n",
    "  autoencoder = Model(input_data, apply_decoder_layers(encoded))\n",
    "\n",
    "  # this model maps an input to its encoded representation\n",
    "  encoder = Model(input_data, encoded)\n",
    "\n",
    "  # create a placeholder for an encoded input\n",
    "  encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "  # create the decoder model\n",
    "  decoder = Model(encoded_input, apply_decoder_layers(encoded_input))\n",
    "\n",
    "  autoencoder.compile(optimizer='adam', loss='mse')\n",
    "  return autoencoder, encoder, decoder\n",
    "\n",
    "\n",
    "def train_and_predict(architecture_f,\n",
    "                      train_data,\n",
    "                      window_size,\n",
    "                      valid_split_size=100, \n",
    "                      epochs=100, batch_size=100, **kwargs): \n",
    "    \"\"\"\n",
    "    The definition of the training process, which will create the autoencoder and\n",
    "    then run training.\n",
    "    Also processes training set into its inner representation in the autoencoder.\n",
    "    Outputs not only the autoencoder, but also the encoder and decoder.\n",
    "    \"\"\"\n",
    "    print(kwargs)\n",
    "    autoencoder, encoder, decoder = architecture_f(window_size=window_size, **kwargs)\n",
    "    \n",
    "    x_train = train_data[:-valid_split_size, :window_size, :].astype('float32')\n",
    "    x_test  = train_data[-valid_split_size:, :window_size, :].astype('float32')\n",
    "    #x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    #x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, x_test))\n",
    "\n",
    "    x_train_pred = encoder.predict(x_train)\n",
    "    x_test_pred = encoder.predict(x_test)\n",
    "    print(\"Predicted data on training and test\")\n",
    "    return x_train_pred, x_test_pred, autoencoder, encoder, decoder\n",
    "\n",
    "\n",
    "x_train_pred, x_test_pred, autoencoder, encoder, decoder = \\\n",
    "    train_and_predict(basic_autoencoder_architecture,\n",
    "                      train_data=train_data,\n",
    "                      window_size=20,\n",
    "                      encoding_dim=800,\n",
    "                      train_data_shape=train_data.shape,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4bduPcs0ttm"
   },
   "source": [
    "# Evaluating and usages basics:  \n",
    "  \n",
    "We have trained an autoencoder. Now lets go through some basic usecases and deduct what is the autoencoder doing! In each case we will also pint out what shoukd be the best non-toy model fo that task. \n",
    "(Can we guess now, what is the toy autoencoder doing?)  \n",
    "  \n",
    "Hints we could know from the architecture are:\n",
    "- Not every autoencoder is suitable for every task, so some of usecases can be misleading in our detective work\n",
    "- The autoencoder is the most basic one, so it would not do fancy stuff\n",
    "  - Analogy in the world of CNNs for images is, that only the deep layers do recognize objects or faces.\n",
    "- The given features are balanced to every character, they should not favor anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DmUI-HX804K"
   },
   "source": [
    "### Looking at how the autoencoder handles a sentence from the training set\n",
    "Real usecases can be data compression, pretraining layers for bigger model, machine translation, or just a test for ruling out bad architectures.  \n",
    "Also there is a class of denoising autoencoders which should not only reconstruct a sentence, but also to repair a modified sentence (reconstruct randomly altered input)\n",
    "*Best used: *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "0J9PrEbV809W",
    "outputId": "4308f016-77fc-4cd3-cdeb-1dc41dc86f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 things from training set:\n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "absolutely dreadful mexican film supposedly based on a short story by edgar allan poe about a newman wanting to go\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "absolute dreadful mencia film supposedly based on a short story by edgar allan poe about a newman wanting to go\n",
      "--mse: 2.042603045993019e-05; ((different))\n",
      "\n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "i found darkness to be just too add ! it had a kind of cool idea and some ambitious ideas\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "i found sanders to be just too rad ! it had a kind of cool idea and some ambitious ideas\n",
      "--mse: 1.6929163393797353e-05; ((different))\n",
      "\n",
      " ... seems pretty OK\n"
     ]
    }
   ],
   "source": [
    "def simply_word_in_embedd_vocab(item, vocab_embedds):\n",
    "  \"\"\"\n",
    "  Lets use the vocabulary search to get just the word from the ()predicted) features.\n",
    "  \"\"\"\n",
    "  search_result = vocab_finder(item, vocab_embedds, key_f=lambda item: item[0],\n",
    "                               apply_on_item_too=False)\n",
    "  # search_result = ((ith position, the vocabulary item), score)\n",
    "  # the vocabulary item is (embedd_f(word), word)\n",
    "  \n",
    "  return search_result[0][1][1]\n",
    " \n",
    "\n",
    "def compare_reconstructed(batch,\n",
    "                          autoencoder=autoencoder,\n",
    "                          emb2text=lambda emb: simply_word_in_embedd_vocab(emb, vocab_embedds),\n",
    "                          verbose=True\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    This function would compare the original sentence to:\n",
    "    - a sentence reconstructed without the autoencoder, just from word features and vocabulary\n",
    "    - a sentence reconstructed using the autoencoder\n",
    "\n",
    "    Also we can measure the MSE of the original and the reconstructed\n",
    "    (without using the vocabulary to separate the effects).\n",
    "    \"\"\"\n",
    "    prediction = autoencoder.predict_on_batch(batch)\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    for pred_item, batch_item in zip(prediction, batch):\n",
    "        word_by_word  = batch_item #np.reshape(batch_item, (-1, train_data.shape[-1]))\n",
    "        orig = u\" \".join([emb2text(word) for word in word_by_word])\n",
    "               \n",
    "        pred_by_word  = pred_item #np.reshape(pred_item, (-1, train_data.shape[-1]))\n",
    "        pred = u\" \".join([emb2text(word) for word in pred_by_word])\n",
    "    \n",
    "        mse = np.mean(np.square(word_by_word - pred_by_word))\n",
    "        if verbose:\n",
    "          print(u\"\\n \\n --Original sentence reconstructed just from vocabulary: \\n\"\n",
    "          \"{}\\n\"\n",
    "          \"--Predicted sentence reconstructed by autoencoder AND vocabulary: \\n\"\n",
    "          \"{}\\n\"\n",
    "          \"--mse: {}; ({})\\n\".format(orig, pred, mse,\n",
    "                                  u\"(different)\" if orig != pred else u\"same\"))\n",
    "        ret.append((orig, pred, mse))\n",
    "    return ret\n",
    "\n",
    "\n",
    "print(\"2 things from training set:\")\n",
    "compare_reconstructed(train_data[0:2, :20])\n",
    "print(\" ... seems pretty OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfAnw2Qz1oEV"
   },
   "source": [
    "### Generating random sentence with the model:\n",
    "Given random inner representation, it should produce a random sentence.  \n",
    "Usefull also for randomly altering a sentence.  \n",
    "\n",
    "*Best used: for non-toy example, more suitable autoencoder architecture should be the VAE (Variational autoencoder), possibly with embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1pA0Qu2ZLUM3",
    "outputId": "aedcbc82-bb9e-4c36-fb1c-e6cf289a11d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting random inner representation as sentence:\n",
      "\n",
      "['l ;) e 2 mockumentary ? attila 52 bambi instantly f n on all-in-all 15 phenomenally n dud antonietta 39']\n",
      "\n",
      "... seems that the autoencoder did not grasp sentence meanings and does not use most common words\n"
     ]
    }
   ],
   "source": [
    "def generate_from_inner(batch,\n",
    "                          decoder=decoder,\n",
    "                          emb2text=lambda emb: simply_word_in_embedd_vocab(emb, vocab_embedds),\n",
    "                          verbose=True\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Given a batch, produces a text, regarding the batch as batch of inner representation features.\n",
    "    \"\"\"\n",
    "    prediction = decoder.predict_on_batch(batch)\n",
    "    ret = []\n",
    "    for pred_item in prediction:\n",
    "        pred_by_word = pred_item # np.reshape(batch_item, (-1, train_data.shape[-1]))\n",
    "        pred = u\" \".join([emb2text(word) for word in pred_by_word])\n",
    "               \n",
    "        ret.append(pred)\n",
    "    return ret\n",
    "\n",
    "print(\"Inspecting random inner representation as sentence:\") \n",
    "print(\"\")\n",
    "zero_input_inner = np.random.rand(*tuple([dim.value if dim.value is not None else 1 for dim in decoder.inputs[0].shape]))\n",
    "print(generate_from_inner(zero_input_inner))\n",
    "print(\"\")\n",
    "print(\"... seems that the autoencoder did not grasp sentence meanings and does not use most common words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n5fd_V_C9icU"
   },
   "source": [
    "### Inspecting a custom sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lANSADhf9OgF"
   },
   "source": [
    "Real world usage would include finding a close sentence in inner representation.  \n",
    "\n",
    "*Best used: In fact it does not need an autoencoder, inner representation can be also taken as the last layer in any trained model - then the closest sentence would be a sentence with closer target values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "n-fPXXYZ1mfY",
    "outputId": "7789b766-cafe-404a-bfff-26b6ad759c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting custom sentence\n",
      "\n",
      "Reconstructed custom sentence (only using vocabulary): \n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "this s film really captured me ! i have enjoyed the starship flying and people jumping all around space !\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "this sf film really captured emy ! i have enjoyed the starship flying and people jumping all around space o\n",
      "--mse: 2.9406851061949293e-05; ((different))\n",
      "\n",
      "Most similar sentence in trained encoder's representation: \n",
      "This is a classic stinker with a big named cast , mostly seniors who were well past their prime and\n",
      "\n",
      " ... ok, works on non training dataset sentence, but the inner representation does not seem to convey any close meaning\n"
     ]
    }
   ],
   "source": [
    "def sentence_into_batch(sentence,\n",
    "                        window_size=20,\n",
    "                        train_data_shape=train_data.shape,\n",
    "                        tokenize_f=text_tokens,\n",
    "                        embedd_f=features_from_text):\n",
    "  \"\"\"\n",
    "  Given a custom sentence, proceses it into a batch to be fed into the model.\n",
    "  \"\"\"\n",
    "  if tokenize_f is None:\n",
    "    tokenize_f = lambda x: x\n",
    "  \n",
    "  custom_b = np.zeros((1, window_size, train_data_shape[-1]))\n",
    "  for i, word in enumerate(tokenize_f(sentence)):\n",
    "      if i >= window_size:\n",
    "          break\n",
    "      custom_b[0, i, :] = embedd_f(word)\n",
    "  return custom_b \n",
    "\n",
    "\n",
    "def inspect_custom_sentence(custom_sentence,\n",
    "                            autoencoder=autoencoder,                            \n",
    "                            encoder=encoder,                            \n",
    "                            embedd_f=features_from_text,\n",
    "                            window_size=20,\n",
    "                            tokenize_f=text_tokens,\n",
    "                            train_data_shape=train_data.shape,\n",
    "                            find_closest_representatios=x_train_pred,\n",
    "                            original_sentences=X_train_orig):\n",
    "    \"\"\"\n",
    "    Given a custom sentence, inspects using compare_reconstructed function and\n",
    "    also tries to find the closest sentence in the training set.\n",
    "    \"\"\"\n",
    "  \n",
    "    custom_b = sentence_into_batch(custom_sentence,\n",
    "                                    window_size,\n",
    "                                    train_data_shape,\n",
    "                                    tokenize_f,\n",
    "                                    embedd_f)\n",
    "\n",
    "    print(\"Reconstructed custom sentence (only using vocabulary): \")\n",
    "    compare_reconstructed(custom_b, autoencoder)  # todo some our words might not be in vocabulary\n",
    "\n",
    "    print(\"Most similar sentence in trained encoder's representation: \")\n",
    "    similar_sentence = vocab_finder(encoder.predict_on_batch(custom_b)[0],\n",
    "                                    find_closest_representatios,\n",
    "                                    None)[0][0]\n",
    "    print(u\" \".join(original_sentences[similar_sentence][0:window_size]))\n",
    "    \n",
    "\n",
    "print(\"Inspecting custom sentence\")  \n",
    "print(\"\")\n",
    "inspect_custom_sentence(u\"This SF film really captured me, \"\n",
    "                        \"I have enjoyed the starships flying \"\n",
    "                        \"and people jumping all around space\")\n",
    "print(\"\")\n",
    "print(\" ... ok, works on non training dataset sentence, but the inner representation does not seem to convey any close meaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leQDUy7s1tgy"
   },
   "source": [
    "### Anomaly detection \n",
    "Lets reconstruct each sentence using the autoencoder and select the most different reconstruction as anomaly.  \n",
    "I have employed some sentences from the training data and some based on my own hypothesis to verify it.\n",
    "![anomaly detection](https://www.spiedigitallibrary.org/ContentImages/Proceedings/10630/1063006/FigureImages/00004_PSISDG10630_1063006_page_4_1.jpg)  \n",
    "(Image source: https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10630/1063006/Deep-learning-based-classification-and-anomaly-detection-of-side-channel/10.1117/12.2311329.short)\n",
    "  \n",
    "Autoencoders are sometimes used to fulfill this role of anomaly detection in financial markets; for that purpose they are retrained often (each 5 minutes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "2KZBL7Dv1jji",
    "outputId": "000edb7d-6a83-4586-f00f-2aa51c065060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting anomalies: (bigger number means bigger anomaly) \n",
      " Last 5 sentences are form the training set, first 3 are custom \n",
      "\n",
      "0.0000587: A wise Klingon once said: hjtcuyf fkudkuyfliug htdjiugh yffkjgkj wsdihga apfgihag agafg iha ah jasth qt FGRIH G.\n",
      "0.0000614: 2018 was my 2. most enjoyable year with films. All 10 films I saw did really count.\n",
      "0.0000452: Aaaaargh! Autoencoders should not be screaaaaming! Aaaaand I think It does it because I waaaant to confuse it!\n",
      "0.0000408: the movie seems disjointed and overall , poorly written . the screenplay moves along as if 10 different people wrote it , and none of them were communicating with each other.\n",
      "0.0000182: dennis hopper is without a doubt one of the finest underrated american actors of our time , and it was interesting to see how he would play out his role as a cop on the case of a child serial killer\n",
      "0.0000186: this movie should not be viewed unless you are trying to kill yourself . i think this movie could actually cause severe brain damage . the main characters are the whiny non - hero kevin , amy.\n",
      "0.0000151: this film reminds me of how college students used to protest against the vietnam war . as if , upon hearing some kids were doing without cheeseburgers in cow dung collehe , the president was.\n",
      "0.0000180: to compare this squalor with an old , low budget flick would be an insult to the old , low budget flick . the animal scenes have no meaning nor do they represent this man and his crimes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspecting anomalies: (bigger number means bigger anomaly) \\n \"\n",
    "      \"Last 5 sentences are form the training set, first 3 are custom \\n\") \n",
    "\n",
    "custom_anomaly_test = [\n",
    "u\"A wise Klingon once said: hjtcuyf fkudkuyfliug htdjiugh yffkjgkj wsdihga apfgihag agafg iha ah jasth qt FGRIH G.\",\n",
    "u\"2018 was my 2. most enjoyable year with films. All 10 films I saw did really count.\",\n",
    "u\"Aaaaargh! Autoencoders should not be screaaaaming! Aaaaand I think It does it because I waaaant to confuse it!\",\n",
    "u\"the movie seems disjointed and overall , poorly written . the screenplay moves along as if 10 different people wrote it , and none of them were communicating with each other.\",\n",
    "u\"dennis hopper is without a doubt one of the finest underrated american actors of our time , and it was interesting to see how he would play out his role as a cop on the case of a child serial killer\",\n",
    "u\"this movie should not be viewed unless you are trying to kill yourself . i think this movie could actually cause severe brain damage . the main characters are the whiny non - hero kevin , amy.\",\n",
    "u\"this film reminds me of how college students used to protest against the vietnam war . as if , upon hearing some kids were doing without cheeseburgers in cow dung collehe , the president was.\",\n",
    "u\"to compare this squalor with an old , low budget flick would be an insult to the old , low budget flick . the animal scenes have no meaning nor do they represent this man and his crimes.\",\n",
    "]\n",
    "\n",
    "anomaly_results = [compare_reconstructed(sentence_into_batch(anomaly),\n",
    "                                        verbose=False)[0] \n",
    "                   for anomaly in custom_anomaly_test]\n",
    "\n",
    "for i, ano_res in enumerate(anomaly_results):\n",
    "  print((\"%.7f\" % ano_res[-1]) + \": \" + custom_anomaly_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mcr6KhwD1F2w"
   },
   "source": [
    "### Evaluating with clustering:\n",
    "- See how a clustering algorithm is able to treat our data, is it able to find any clusters?\n",
    "- How do we evaluate clusters? Lets just look at some sentences from a noncommon cluster, if they have the same theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ofH-Q2G6wN9V",
    "outputId": "6738878e-2a2c-40bd-f674-6596b3f1de5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing clusters... from 4900 datapoints\n",
      "Cluster count: 75\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html\n",
    "def compute_clusters(features, threshold=0.9):\n",
    "    print('Computing clusters... from {} datapoints'.format(features.shape[0]))\n",
    "    # `n_clusters=None` means variable number of clusters\n",
    "    # we control clustering by the threshold\n",
    "    cluster_algo = Birch(n_clusters=None, threshold=threshold)\n",
    "    # predicted cluster labels (typically integers from 0)\n",
    "    clusters = cluster_algo.fit_predict(features)\n",
    "    cluster_count = len(set(clusters))\n",
    "    print('Cluster count:', cluster_count)\n",
    "    return clusters\n",
    "\n",
    "ret_clust = compute_clusters(x_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "pLKlgdNdwSUn",
    "outputId": "ba944d3b-f02d-4d31-a33f-91aca2249d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting some small cluster:\n",
      "\n",
      "[52, 219, 259, 478, 829, 1787, 3058, 3292, 4257, 4464]\n",
      "\n",
      "id 52\n",
      "If the myth regarding broken mirrors would be accurate , everybody involved in this production would now face approximately 170 years of bad luck , because there are a lot of mirrors falling to little pieces here . If only the script was as shattering as the glass , then \" The Broken \" would have been a brilliant film . Now it ' s sadly just an overlong , derivative and dull movie with only just a handful of remarkable ideas and memorable sequences . Sean Ellis made a very stylish and elegantly photographed movie , but the story is lackluster and the total absence of logic and explanation is really frustrating . I got into a discussion with a friend regarding the basic concept and \" meaning \" of the film . He thinks Ellis found inspiration in an old legend claiming that spotting your doppelganger is a foreboding of how you ' re going to die . Interesting theory , but I ' m not familiar with this legend and couldn ' t find anything on the Internet about this , neither . Personally , I just think \" The Broken \" is yet another umpteenth variation on the theme of \" Invasion of the Body Snatchers \" but without the alien interference .\" The Broken \" centers on the American McVey family living in London , and particularly Gina . When a mirror spontaneously breaks during a birthday celebration , this triggers a whole series of mysterious and seemingly supernatural events . Gina spots herself driving by in a car and follows her mirror image to an apartment building . Whilst driving home in a state of mental confusion , she causes a terrible car accident and ends up in the hospital . When dismissed , Gina feels like her whole surrounding is changing . She doesn ' t recognize her own boyfriend anymore and uncanny fragments of the accident keep flashing before her eyes . Does she suffer from mental traumas invoked by the accident or is there really a supernatural conspiracy happening all around her ? Writer / director Sean Ellis definitely invokes feelings of curiosity and suspense in his script , but unfortunately he fails to properly elaborate them .\" The Broken \" is a truly atmospheric and stylish effort , but only after just half an hour of film , you come to the painful conclusion it shall just remain a beautiful but empty package . There ' s a frustratingly high amount of \" fake \" suspense in this film . This means building up tension , through ominous music and eerie camera angels , when absolutely nothing has even happened so far . By the time the actually mysteriousness kicks in , these tricks don ' t have any scary effect on you anymore . Some of my fellow reviewers around here compare the film and particularly Sean Ellis ' style with the repertoires of David Lynch , Stanley Kubrick and even Alfred Hitchcock , but that is way , way  WAY too much honor . PS : what is up with that alternate spelling ; the one with the Scandinavian \" ø \"\n",
      "\n",
      "id 219\n",
      "The latest Rumor going around is that Vh1 is starting casting calls for I Love New York 3 mid 2008 . So does this mean Budah or Tailor made dumped New York or does this mean New York dumped the winner ? I know Flavor of Love is coming up to it ' s 3rd season , so now with a Flavor of Love 3 and a I love New York 3 ..... will there ever be a true winner ??? I ' ve also heard a few rumors that Chance WILL be brought back for the 3rd Season of I Love New York !!!! I have also heard rumors that New York will be Specially featured on Flavor of Love 3 . Hopefully this was not too much of a spoiler for the ending of I Love New York 2 .... I ' m just stating the latest rumor .\n",
      "\n",
      "id 259\n",
      "One of the most popular rentals at my local video store is not Borat or The Departed but a 2005 documentary about Jesus Christ called The God Who Wasn ' t There by director Brian Flemming , an ex - Christian Fundamentalist . Flemming , in his 62 - minute documentary , asserts that Jesus was not a historical figure but a legend based solely on Pagan traditions . Using interviews with authors , philosophers , and historians to debunk the long - held Christian belief that Jesus , the son of God , lived among men , was crucified , and was resurrected , Flemming compares the Christ story with those of cult figures Isis and Osiris in Egypt , Dionysus and Adonis in Greek mythology , and Roman mystery cults such as Mithraism and finds many surprising similarities . In addition to his evidence about Pagan cults , he also states that the earliest sources for the Christ story , the four gospels , were written forty or fifty years after the date given for Jesus ' crucifixion and that the letters of St . Paul show little evidence of Jesus being a flesh and blood figure . Flemming , unfortunately however , is not out to conduct a solid investigation of the truth about Jesus ' life but to use the subject only as a point of departure for a full throttle attack on Christianity and all religion . Most of the interviews are with those philosophically aligned with the director including avowed atheists such as Biologist Richard Dawkins and author Sam Price . The only Christians interviewed are those on the fringe such as Scott Butcher , the creator of the website Rapture Letters . com , and Ronald Sipus , principal of the fundamentalist Village Christian School , which Flemming attended as a boy . Like Michael Moore ' s interview of Charlton Heston in Bowling for Columbine , his interview with Sipus is so contentious that Sipus walks out in the middle . In a sarcastic tone , Flemming tells us how wrong Christianity was wrong about the sun revolving around the earth , then points to atrocities committed in the name of Christianity such as those by cult leader Charles Manson who killed 11 people and Dena Schlosser , who cut her baby ' s arm off for God . He also lifts a statement from a book by LaHaye and Jenkins that says that Christians \" look forward to the day when all non - Christians are thrown into a lake of fire , howling and screeching .\" To further turn us against Christianity , Flemming shows us extended clips from Mel Gibson ' s The Passion of the Christ , detailing in minute detail each scene of violence and torture . What could have been a serious discussion on a very interesting subject eventually becomes a childish rant and a polemic against all religion . In the process of condemning those who used Christianity to commit unspeakable acts , he ignores such people as socialist Muriel Lester , a famous Christian pacifist , Rigoberta Menchú Tum , a Mayan Indian of Guatemala who helped found the Revolutionary Christians and received the Nobel peace prize in recognition of her work for social justice , and Mother Teresa , whose work was about respect for each individual ' s worth and dignity . His most telling argument is his comparison of Christian doctrine with the Pagan cults and he makes some good points , yet Flemming does not tell us that while some aspects of these cults may resemble Christian doctrines , there are no texts or source materials for these cults before 300AD , long after the New Testament . Also it is important to note one major difference . The immediate goal of the initiates was a mystical experience that led them to feel they had achieved union with their god . This is anathema to Christianity which believes that a Church hierarchy including priests and bishops all the way up to the Pope are required to interpret God ' s will to mankind . Although I am not a Christian and have some doubts about whether or not Jesus Christ was in fact a historical figure , the truth is that , in the long scheme of things , it may not matter . What matters is that a message was introduced to mankind and spread around the world that contributed to mankind ' s spiritual evolution . Regardless of the distortions and crimes later committed in its name and there were many , Christianity as conceived was a doctrine of compassion and love , and a moral and ethical code that furthered respect for our fellow man . While I applaud the fact that the film was made and that a taboo topic was discussed , what is sorely needed is not another divisive attempt to use religion as a field of combat but to see it as a common thread that can bring the world ' s people together . While there is room for debate and discussion on religious subjects , in the words of Annie Besant ,\" spiritual truths are best seen in the clear air of brotherhood and mutual respect . The God Who Wasn ' t There is recommended only for those whose idea of a good time is to trash the religion of others .\n",
      "\n",
      "id 478\n",
      "This review is based on the dubbed Shock - o - Rama video released on an undeserving world in 2002 . How bad is it ? It ' s awful , which is what a ' 1 ' represents on the IMDb scale -- but it ' s much worse than that . It ' s nice to imagine that an original German - language print might improve matters -- the comedic English - language dubbing isn ' t funny at all -- but truthfully , this is one of the worst amateur films of any genre you ' re likely to see . The zombies in the film are as slow and clumsy as ever , and they don ' t seem to have the ability to speak or think about anything beyond their next meal . However , they ' re also intelligent enough to operate chainsaws and malicious enough to know that western taboos about genitalia will no doubt enliven their dinner table conversation . George Romero ' s Land of the Dead posited a zombie nation that retained a shred of social coherence ; here , zombies are nothing more than an empty canvas for the perverse imaginings of director Andreas Schnaas . Utterly without redeeming social value , and even worse , entirely lacking as entertainment , Zombie ' 90 is a bad joke on anyone who wastes money on it .\n",
      "\n",
      "id 829\n",
      "A very young Ginger Rogers trades quick quips and one liners with rival newspaper reporter Lyle Talbot in this 1933 murder mystery from Poverty Row film maker Allied Productions . The movie opens with a wealthy businessman taking a header from the roof garden of a high rise apartment house , or was it from a lover ' s apartment ? Rogers actually has two identities at the film ' s outset , that of Miss Terry , the dead victim ' s secretary , along with her newspaper byline of Pat Morgan . Mistakenly phoning her story directly to Ted Rand ( Talbot ) instead of her paper ' s rewrite desk , she gets fired for her efforts when her boss learns he ' s been out scooped . Here ' s a puzzle - it ' s revealed during Police Inspector Russell ' s ( Purnell Pratt ) investigation of Harker ' s death that Terry / Morgan had been employed as his secretary for three weeks . Why exactly was that ? After the fact it would make sense that she was there for a newspaper story , but before ? Clues are dropped regarding Harker ' s association with a known mobster conveniently living in the same apartment building , but again , that association isn ' t relevant until it ' s all linked up to janitor Peterson ( Harvey Clark ). And who ' s making up all the calling cards with the serpent effecting a HSSS , with the words \" You will hear it \" cut and pasted beneath ? Apparently , the hissing sound of a snake was the sound made by the apartment house ' s radiator system , which Peterson used to transmit a poisonous gas into the rooms of potential victims , such as Mrs . Coby in the apartment below Harker . But in answer to a question posed to Inspector Russell about Mrs . Coby ' s death , he replied \" apparently \" to the cause of strangulation . It ' s these rather conflicting plot points that made the movie somewhat unsatisfying for me . The revelation of janitor Peterson as the bad guy of this piece comes under somewhat gruesome circumstances as we see him stuff the unconscious body of Miss Morgan in the building ' s incinerator furnace ! However , and score another point against continuity , we see Miss Morgan in a huge basement room as Peterson ignites the furnace ; she made her getaway , but how ? And still pretty as a picture . And who gets to make the collar off screen if none other than milquetoast police assistant Wilfred ( Arthur Hoyt ), who in an opening scene fell over his own feet entering a room . Sorry , but for all those reviewers who found \" A Shriek in the Night \" to be a satisfying whodunit , I feel that any Charlie Chan film of the same era is a veritable \" The Usual Suspects \" by comparison . If you need a reason to see the film , it would be Ginger Rogers , but be advised , she doesn ' t dance .\n",
      "\n",
      "id 1787\n",
      "I will keep this as short as possible as this piece of crap barely warrants a mention . ZOMBIE 90 is one of the worst films ever made - right up there with Schnaas ' other horrible zombie entry - ZOMBIE DOOM ( aka VIOLENT SH ! T 3 ). These films suck so bad that everyone involved in their creation should be shot . I somehow managed ( barely ...) to sit through ZOMBIE DOOM - but ZOMBIE 90 is so horribly inept - even when compared to Schnaas ' other horrible film - that I had to fast - forward through everything after the first 10 minutes . ZERO acting skills , inept gore , horrible camcorder - style camera - work , ridiculous dubbing ... it just goes on and on . I really can ' t find a single thing redeeming about this garbage - and I can usually find SOMETHING redeeming in just about ANY film . This truly is one of the worst films ever made - You ' ve been warned ... 1 / 10\n",
      "\n",
      "id 3058\n",
      "Disappointing musical version of Margaret Landon ' s \" Anna and the King of Siam \", itself filmed in 1946 with Irene Dunne and Rex Harrison , has Deborah Kerr cast as a widowed schoolteacher and mother who travels from England to Siam in 1862 to accept job as tutor to the King ' s many children -- and perhaps teach the Royal One a thing or two in the process ! Stagy picture begins well , but quickly loses energy and focus . Yul Brynner , reprising his stage triumph as the King , is a commanding presence , but is used -- per the concocted story -- as a buffoon . Kerr keeps her cool dignity and fares better , despite having to lip - synch to Marni Nixon ' s vocals . Perhaps having already played this part to death , Brynner looks like he had nothing leftover for the screen translation except bombast . Second - half , with Anna and the moppets staging a musical version of \" Uncle Tom ' s Cabin \" is quite ridiculous , and the Rodgers and Hammerstein songs are mostly lumbering . Brynner won a Best Actor Oscar , but it is feisty Kerr who keeps this bauble above water . Overlong , heavy , and ' old - fashioned ' in the worst sense of the term .** from ****\n",
      "\n",
      "id 3292\n",
      "This was a very brief episode that appeared in one of the \" Night Gallery \" show back in 1971 . The episode starred Sue Lyon ( of Lolita movie fame ) and Joseph Campanella who play a baby sitter and a vampire , respectively . The vampire hires a baby sitter to watch his child ( which appears to be some kind of werewolf or monster ) while he goes out at night for blood . The baby sitter is totally oblivious to the vampire ' s appearance when she first sees him and only starts to put two and two together when she notices that he has no reflection in the mirror , has an odd collection of books in the library on the occult , and hears strange noises while the vampire goes to talk to the child . She realizes that the man who hired her may not be what she thought he was originally . She bolts out the door , the vampire comes out looking puzzled and the episode is over . I don ' t know what purpose it was to make such an abbreviated episode that lasted just 5 minutes . They should just have expanded the earlier episode by those same 5 minutes and skipped this one . A total wasted effort .\n",
      "\n",
      "id 4257\n",
      "( possible spoilers ) Someone once asked Dr . Seuss if they could secure the movie rights to his 1957 Christmas classic How the Grinch Stole Christmas . He turned them down , insisting that no one could do better than the marvelous Chuck Jones TV special from 1966 ( also in mind , perhaps , was his bitter experience writing the script to 1953 ' s The 5 , 000 Fingers of Dr . T ). When the good Dr . died in 1991 , his widow , Audrey Geisel , still obstinately refused to sell the movie rights . But with the commonplace use of CGI effects becoming a reality , Mrs . Geisel had a change of heart . Universal made her a generous offer she accepted ; she also accepted the casting of Jim Carrey as the title character . Supposedly she was satisfied with the final result . Well , Mrs . Geisel , that makes one of us . The film was given a $ 123 , 000 , 000 budget ( which is more than even Heaven ' s Gate cost , including the adjustment for inflation ), which obviously went towards the very elaborate makeup , set design , and special effects ( which are undermined  somewhat by the rather hazy cinematography ). Unfortunately , it seems that none of that money was set aside to get a better script than what Jeffrey Price and Peter S . Seaman ( scribes of Who Framed Roger Rabbit ?, which made much better  use of a high budget ) turned in . Whereas the TV special was a trim 26 minutes without commercials , this film tries to fill a running time of 105 minutes with more background information about the Grinch . It turns out that , as a child , he was the subject of ridicule , including an especially humiliating experience one Christmas at the age of eight . So it turns out that everything that ails our poor Mr . Grinch is directly because of the Whos . Trouble is , it seems like a rather long 105 minutes , with too much dead wood clogging up the story . That might not seem so bad if only the Grinch were a little more ... well , Grinchy . The character that Dr . Seuss wrote and Chuck Jones later animated was a sly fox whose slick attempts to hijack the holiday season were undermined by his sudden change ( and exponential growth ) of heart . Carrey ' s Grinch is a loud , hyperactive oaf and , at times , a thug who , when made the holiday ` Cheermeister ,' trashes the Whoville town square in anger ( hopefully the scenery tasted as good as it looked ). This undermines the script ' s attempt to make the Grinch more sympathetic , with all the Whos down in Whoville so unsympathetic ( at least in this interpretation ). The Whoville of Dr . Seuss ' s vision was a small town populated by honest folk who knew in their hearts the true meaning of Christmas . The Whoville of the movie is a rather noisy and crowded place populated by spoiled , selfish , materialistic ninnies ; an obvious attempt to comment on American consumerism . This is offensively  hypocritical inasmuch as the film industry has benefitted greatly from American consumerism , and as this film contributed to it with a huge merchandising  campaign . The film also expands and redefines the character of Cindy Lou Who , a small but crucial character in the original . The innocent two - year - old waif who walked in on this spurious Santa is now older and wiser , constantly questioning the false values of the Whos and trying to understand the Grinch ' s point of view ( her one major scene from the original is re - enacted , making it seem out of character ). She  seems to be the only one who would ever know that Christmas is more than just gifts and decorations , thus making her a completely different , and more annoying , character . Those who celebrate Christmas should sooner accept a lump of coal in their  stockings on the morning of December 25 than a copy of this overlong , overacted , excruciatingly tedious , ham - handed , crude attempt to turn the children ' s classic into a feature film . It proves once and for all that darkness , vulgarity , manipulation , and heavy - handedness are inadequate substitutes for charm , wit , sincerity , and heart . The folks at Universal should get down on their collective knees and thank God that this truly bilious $ 123 million stink bomb grossed $ 260 million domestically or they ' d not be here today . Furthermore it made Mike Myers ' The Cat in the Hat possible !\n",
      "\n",
      "id 4464\n",
      "This early Adam Sandler film could be compared to his life as a comic during the same period in 1989 . His character ' s constant acknowledgement of his hidden comic genius and frustration regarding humorous material seems to come more from Sandler than the script . The film is nothing compared to his blockbuster feature films , such as Big Daddy or even the corny Billy Maddison . Unfortunately , Sandler had not yet found a way to express himself in a consistent , successful and funny manner when this film was made , much like his character . The majority of the film ' s \" jokes \" come from Sandler having conversations with himself , usually over his unrecognised comic talent and beating himself up because he ' s too ugly and can ' t get women . The film is hard to watch too because it doesn ' t treat itself like a real film . Sandler talks to the camera and the viewers throughout the film , often referring to the film ' s low budget or questionable content . The film is ultimately awkward and embarrassing to watch . I immediately wanted to forget I even saw this film after it was over , for fear that if more found out about it , it would ruin Sandler ' s career . Pass this one up at the video store , I rented it for free and it was still a waste of time .\n",
      "\n",
      "Ok notice the numbers and their positions in the sentence!\n"
     ]
    }
   ],
   "source": [
    "def print_cluster_example(cluster_i=0, n_examples=10):\n",
    "  ids = [i for i, clust in enumerate(ret_clust) if clust == cluster_i]\n",
    "  ids = ids[:n_examples]\n",
    "  print(ids)\n",
    "  for oid in ids:\n",
    "      print(u\"\")\n",
    "      print(u\"id {}\".format(oid))\n",
    "      print(u\" \".join(X_train_orig[oid]))\n",
    "      \n",
    "def find_less_count_clusters():\n",
    "  from collections import Counter\n",
    "  rets = Counter(ret_clust)\n",
    "  for key in rets.keys():\n",
    "    if rets[key] <= 10 and rets[key] >= 3:\n",
    "      return key\n",
    "  \n",
    "print(\"Inspecting some small cluster:\")\n",
    "print(\"\")\n",
    "print_cluster_example(cluster_i=find_less_count_clusters(), n_examples=10)\n",
    "print(\"\")\n",
    "print(\"Ok notice the numbers and their positions in the sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "ZR25TkjWAWWq",
    "outputId": "6a8e0e95-4a43-4d00-b0e5-9efb5dc8282f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the clusters looking? Lets just inspect them with TSNE for visualization: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"How are the clusters looking? Lets just inspect them with TSNE for visualization: \")\n",
    "Y = tsne(np.float64(x_train_pred[0:5000]))\n",
    "plt.scatter(Y[:, 0], Y[:, 1])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "If we have used a variational autoencoder, it coud make sense to display just the inner representation:\n",
    "print(\"Here we will take the first two coordinates from the inner representation and display it\"\n",
    "      \"(useful when using variational autoencoder or autoencoder with just 2 units)\")\n",
    "plt.scatter(x_train_pred[:, 0], x_train_pred[:, 1])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "UZ0b9z2GLUtJ",
    "outputId": "ea671bf9-71b3-4715-e2a7-32d8ce735047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea: We could inspect the small clusters in the TSNE representation: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"A disappointing film . The story established our protagonist as Chrissy , a ' young ', rather sullen individual drifting , not doing much . Actually she does very little to move the narrative along so it didn ' t surprise me to see the focus shifting on her relatives . It ' s a pity though , Chrissy seem like interesting character . Story was predictable and at times felt quite formulated . So the question now is , when are we going to see the Campions , Jacksons , and the Tamahori ' s breaking ground with compelling , cinematically - told stories that will inspire , rather than entertain for the toll of two hours ? Technically , a disgusting shot film .\",\n",
       " 'My girlfriend has the habit of going to Blockbuster and choosing movies no - one has ever heard anything about . Admittedly , at times , it has led to some fun discoveries . Often times , the best that can be said is they definitely run an hour and a half . She brought home \" Advice From A Caterpillar .\" She was excited because the box said it was funny . Lucky for us , the propaganda on the boxes never lie . This movie was an exercise in patience . This is one of those movies where , unless you are a pretentious and shallow person who likes watching movies about yourself , you will hate every character in the movie . Until the introduction of the one nice character . Which the lead annoying pretentious character will fall in love with and act in such a way that , in the real world , would drive anyone away . MILD SPOILERS FROM HERE ON  So a bunch of emotionally vapid , stuck - up , pretentious artists swear off love and find success in their careers . Then , they meet a nice , intelligent , emotionally mature and loving character ( an almost perfect guy ). We then watch the woman , the annoyingly pretentious artist ( in her 30 \\' s ?) freak out as she falls in love . So she tries to flee from the nice , intelligent , emotionally mature man and stay with the married man with whom she \\' s been having great but empty sex . She is rude to the man and does everything in her power to drive him away . In the real world , she would have been quite successful . I certainly wanted to flee from her and I wasn \\' t even in a relationship with her ! Although its nice that the man \\' fought for his love \\', I never wanted her to have him .( Nor did my girlfriend ) She didn \\' t deserve him . And , why I wonder , did the director think that the \\' almost perfect guy \\' should be punished by having to win a relationship with her ? When the artist was asking the \\' almost perfect guy \\' to leave , we were screaming for him to leave too . There \\' s a problem with a movie when the heroine of the film is so annoying , childish and stupid that you want her to fail . Beyond that , let me say that Andy Dick made me laugh a few times even though his character was also pretentious to the point of annoyance . Regarding the other characters , they were well acted , morally bankrupt and annoying characters . It is a comedy and I can say I did laugh a few times in the film . Unfortunately , not much laughing happened until the last 10 minutes or so . But by the time I had those laughs , I had been praying for the movie to end for far too long . I needed to get these vapid characters out of my life . If you want to watch people you hate struggle with a love for people they don \\' t deserve , then this is the movie for you .',\n",
       " \"DO NOT WATCH THIS MOVIE IF YOU LOVED THE CLASSICS SUCH AS TOM WOPAT , JOHN SCHNEIDER , CATHERINE BACH , SORRELL BOOKE , JAMES BEST , DENVER PYLE , SONNY SHROYER , AND BEN JONES ! THIS MOVIE WILL DISSAPPOINT YOU BADLY ! First of all , this movie starts out with Bo and Luke running moonshine for Jesse . Bo and Luke would not do that ever on the real series ! This movie portrays unimaginable characters doing things that never would have happened in the series . In the series , Uncle Jesse was honest , and law - abiding . In this movie , he is a criminal who is making moonshine and smoking weed with the governor of Georgia . Plus , if this was an extension adding on to the Dukes of Hazzard Reunion ! and the Dukes of Hazzard in Hollywood , I have one question : HOW COULD UNCLE JESSE BE MAKING MOONSHINE WHEN HE DIED BEFORE THE DUKES OF HAZZARD IN Hollywood MOVIE ? AND HOW IS BOSS HOGG ALIVE WHEN HE DIED BEFORE THE REUNION MOVIE IN 1997 ! MOVIE AND ROSCO RAN HAZZARD ? IT SEEMS MAGICAL THAT THESE CHARACTERS CAME BACK TO LIFE , WHEN THEY HAVE BEEN DEAD FOR 11 AND 8 YEARS ? If Hollywood really wanted to make a good movie , they should have brought back James Best , John Schneider , Tom Wopat , Ben Jones , and Catherine Bach like they did in 1997 and 2000 and made a family friendly movie with the living original characters that made the show what it was and still is compared to this disgusting , disgraced movie ! If you want to see good Dukes movies , either buy the original series , or go out to walmart . com and buy the DVD set of 2 that includes the Reunion , and Dukes of Hazzard in Hollywood movies ! They both star the original cast , and are family friendly ! Don ' t waste your time on a movie that isn ' t worth the CD it ' s written on !\",\n",
       " \"Small college town coed OD ' s ?( Why do we care ?) Acting sheriff investigates the incident .( Why do we care ?) The interviews show us the comatose subject ( Kirshner ) as different as the opinions of the subjects being interviewed .( Why do we care ?) Result ? A mess of flashbacks in this mess of a movie featuring a handful of one - hit wonders and B - flick divas which begs the question ... Why do we care ?\",\n",
       " \"Before starting to watch the show , I ' ve heard it was great and aesthetically very interesting . What a deception , the scripts are so dumb that I am quite sure the authors are son and grandson of Scoobidoo writers . And what about the SFX and colors , they are so extreme that it is painful to watch , colors are not saturated they are over saturated , like scripts are overwritten and show is overrated . This show is like a bad pie in which a child would have put only sugar and butter thinking that because these ingredients are the best , they are sufficient . Unfortunately for this show , the only two ingredients of this show are finally vacuity and a total lack of credibility .\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Idea: We could inspect the small clusters in the TSNE representation: \\n\")\n",
    "point_inspect = [30, -20]  # input your own point\n",
    "\n",
    "dists = [(i, (point_inspect[0]-item[0])*(point_inspect[0]-item[0]) +\n",
    "          (point_inspect[1]-item[1])*(point_inspect[1]-item[1])) \n",
    "         for i, item in enumerate(Y)]\n",
    "dists.sort(key=lambda item: item[1])\n",
    "[\" \".join(X_train_orig[item[0]]) for item in dists[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTXoWIVavvhI"
   },
   "source": [
    "## Conclusion and more ideas for experiments:\n",
    "- Spoiler: **Even though the textual features use numbers, the autoencoder found them to not be so much frequent and decided they are the biggest anomalies.**\n",
    "- Real life purpose of this autoencoder could then be: **To find a text, that has unusual distribution of characters.**\n",
    "  - Could be upgraded to ignore numbers altogether, then it would process only text and find the outliers.\n",
    "  - *Remember, this is just an example with easiest autoencoder architecture. For real life applications, different architectures and embeddings/ character models should be used. Also it is possible to use a trained model and inner representation can be its last layer.*\n",
    "  \n",
    "Ways to experiment with the code:\n",
    "- The code can be tried out with embeddings\n",
    "- Character level architectures can be used\n",
    "- Varying the parameters can lead the model to perform differently\n",
    "- Or empolying different model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25E4cqlx_UR5"
   },
   "source": [
    "### When we know the purpose of the autoencoder, we can search the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "UaSo9c8p91My",
    "outputId": "91e74ca8-eb73-45db-e869-f712e9706475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me sentence with this distribution of o's\n",
      "\n",
      "Reconstructed custom sentence (only using vocabulary): \n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "cookbook ! i use a lookout of sooooo ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "notebook ! i use a lookout of sooooo ! o o a a t a e a ! e !\n",
      "--mse: 6.572770226909424e-05; ((different))\n",
      "\n",
      "Most similar sentence in trained encoder's representation: \n",
      "Somebody owes Ang Lee an apology . Actually , a lot of people do . And I ' ll start\n",
      "\n",
      "\n",
      "\n",
      "Give me sentence with numbers inside\n",
      "\n",
      "Reconstructed custom sentence (only using vocabulary): \n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "1 2 3 4 5 6 7 8 9 10 11 ;) you know that aim called the count !\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "19 23 2 0 8 9 9 6 9 16 21 ! yo note that aim laced the count !\n",
      "--mse: 0.00012644489065734234; ((different))\n",
      "\n",
      "Most similar sentence in trained encoder's representation: \n",
      "My Score for this crap : 1 / 10 1 for the technical only . Everything else is very bad\n"
     ]
    }
   ],
   "source": [
    "print(\"Give me sentence with this distribution of o's\")\n",
    "print(\"\")\n",
    "inspect_custom_sentence(u\"Noooooooo! I use a looooot of Ooooooos!\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Give me sentence with numbers inside\")\n",
    "print(\"\")\n",
    "inspect_custom_sentence(u\"1 2 3 4 5 6 7 8 9 10 11 ... You know that iam called the Count! Because I really love to count!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "jV1qrG_V-82b",
    "outputId": "4b6d1839-303f-4dbe-fe0b-5f868b3cb0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me sentence to train my pronounciation!\n",
      "\n",
      "Reconstructed custom sentence (only using vocabulary): \n",
      "\n",
      " \n",
      " --Original sentence reconstructed just from vocabulary: \n",
      "waqt waqt waqt waqt waqt waqt waqt q w r t q w r t q w r t !\n",
      "--Predicted sentence reconstructed by autoencoder AND vocabulary: \n",
      "whit wore west west wire twin wait t w r t b w r t i w r t w\n",
      "--mse: 3.4385971133244184e-05; ((different))\n",
      "\n",
      "Most similar sentence in trained encoder's representation: \n",
      "Many people here say that this show is for kids only . Hm , when I was a kid (\n"
     ]
    }
   ],
   "source": [
    "print(\"Give me sentence to train my pronounciation!\")\n",
    "print(\"\")\n",
    "inspect_custom_sentence(u\"qwrt qwrt qwrt qwrt qwrt qwrt qwrt q w r t q w r t q w r t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOMlOGRqFo8n"
   },
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWJbvqjKEiZ2"
   },
   "source": [
    "Links for literature and more examples (not only autoencoders, but also other visualizations used here):   \n",
    "\n",
    "\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html  \n",
    "https://lvdmaaten.github.io/tsne/  \n",
    "https://github.com/lvdmaaten/bhtsne/  \n",
    "https://cs224d.stanford.edu/reports/OshriBarak.pdf  \n",
    "https://hackernoon.com/how-to-autoencode-your-pok%C3%A9mon-6b0f5c7b7d97   \n",
    "https://datascience.stackexchange.com/questions/29527/which-type-auto-encoder-gives-best-results-for-text   \n",
    "https://www.reddit.com/r/MachineLearning/comments/2em084/el5_what_are_autoencoders_used_for/  \n",
    "https://www.siarez.com/projects/variational-autoencoder  \n",
    "IMDB dataset for sentient:\n",
    "https://github.com/rossumai/mlprague18-nlp/blob/master/IMDB%20Sentiment.ipynb\n",
    "  \n",
    "  Bonus for everybody - colab notebook with character model (convolutional autoencoder): https://colab.research.google.com/drive/1DfLYljazmkxAwBCRt1AHbdtBU3xV0lPr ;) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rossum Autoencoders and texts",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
